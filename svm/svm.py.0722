'''
支持向量机 概述
支持向量机(Support Vector Machines, SVM)：是一种监督学习算法。
    1. 支持向量(Support Vector)就是离分隔超平面最近的那些点。
    2. 机(Machine)就是表示一种算法，而不是表示机器。
SVM 工作原理
    1. 寻找最大分类间距
    2. 转而通过拉格朗日函数求优化的问题
        1.数据可以通过画一条直线就可以将它们完全分开，
        这组数据叫线性可分(linearly separable)数据，而这条分隔直线称为分隔超平面(separating hyperplane)。
        2.如果数据集上升到1024维呢？那么需要1023维来分隔数据集，
        也就说需要N-1维的对象来分隔，这个对象叫做超平面(hyperlane)，也就是分类的决策边界。

怎么寻找最大间隔
点到超平面的距离
    分隔超平面函数间距: y(x)=w^Tx+b
    分类的结果： f(x)=sign(w^Tx+b) (sign表示>0为1，<0为-1，=0为0)
    点到超平面的几何间距: d(x=(w^Tx+b)/||w||
    (||w||表示w矩阵的二范数=> sqrt{w^T*w}, 点到超平面的距离也是类似的)

拉格朗日乘子法
    类别标签用-1、1，是为了后期方便 lable∗(wTx+b)lable∗(wTx+b) 的标识和距离计算；如果 lable∗(wTx+b)>0lable∗(wTx+b)>0 表示预测正确，否则预测错误。
    现在目标很明确，就是要找到w和b，因此我们必须要找到最小间隔的数据点，也就是前面所说的支持向量。
    也就说，让最小的距离取最大.(最小的距离：就是最小间隔的数据点；最大：就是最大间距，为了找出最优超平面--最终就是支持向量)
    目标函数：arg:max关于w,b(min[lable∗(wTx+b)]∗1||w||)arg:max关于w,b(min[lable∗(wTx+b)]∗1||w||)
    如果 lable∗(wTx+b)>0lable∗(wTx+b)>0 表示预测正确，也称函数间隔，||w||||w|| 可以理解为归一化，也称几何间隔。
    令 lable∗(wTx+b)>=1lable∗(wTx+b)>=1， 因为0～1之间，得到的点是存在误判的可能性，所以要保障 min[lable∗(wTx+b)]=1min[lable∗(wTx+b)]=1，才能更好降低噪音数据影响。
    所以本质上是求 arg:max关于w,b 1/||w||；也就说，我们约束(前提)条件是: lable∗(wTx+b)=1lable∗(wTx+b)=1
    新的目标函数求解： arg:max关于w,b 1/||w||
    => 就是求: arg:min关于w,b ||w|| (求矩阵会比较麻烦，如果x只是 1/2∗x^2 的偏导数，那么。。同样是求最小值)
    => 就是求: arg:min关于w,b(1/2∗||w||^2) (二次函数求导，求极值，平方也方便计算)
    本质上就是求线性不等式的二次优化问题(求分隔超平面，等价于求解相应的凸二次规划问题)
    通过拉格朗日乘子法，求二次优化问题
    假设需要求极值的目标函数 (objective function) 为 f(x,y)，限制条件为 φ(x,y)=M # M=1
    设g(x,y)=M-φ(x,y) # 临时φ(x,y)表示下文中 label∗(wTx+b)label∗(wTx+b)
    定义一个新函数: F(x,y,λ)=f(x,y)+λg(x,y)
    a为λ（a>=0），代表要引入的拉格朗日乘子(Lagrange multiplier)
    那么： L(w,b,α)=1/2∗||w||^2+∑ni=1αi∗[1−label∗(wTx+b)]L(w,b,α)=1/2∗||w||2+∑i=1nαi∗[1−label∗(wTx+b)]
    因为：label∗(wTx+b)>=1,α>=0label∗(wTx+b)>=1,α>=0 , 所以 α∗[1−label∗(wTx+b)]<=0α∗[1−label∗(wTx+b)]<=0 ,
     ∑ni=1αi∗[1−label∗(wTx+b)]<=0∑i=1nαi∗[1−label∗(wTx+b)]<=0
    相当于求解： max关于αL(w,b,α)=1/2∗||w||^2
    如果求： min关于w,b 1/2∗||w||^2, 也就是要求： min关于w,b(max关于αL(w,b,α))min关于w,b(max关于αL(w,b,α))
    现在转化到对偶问题的求解
    min关于w,b(max关于αL(w,b,α))min关于w,b(max关于αL(w,b,α)) >= max关于α(min关于w,b L(w,b,α))max关于α(min关于w,b L(w,b,α))
    现在分2步
    先求： min关于w,bL(w,b,α)=1/2∗||w||^2+∑ni=1αi∗[1−label∗(wTx+b)]min关于w,bL(w,b,α)=1/2∗||w||^2+∑i=1nαi∗[1−label∗(wTx+b)]
    就是求L(w,b,a)关于[w, b]的偏导数, 得到w和b的值，并化简为：L和a的方程。


SMO 高效优化算法
SVM有很多种实现，最流行的一种实现是： 序列最小优化(Sequential Minimal Optimization, SMO)算法。
下面还会介绍一种称为核函数(kernel)的方式将SVM扩展到更多数据集上。
注意：SVM几何含义比较直观，但其算法实现较复杂，牵扯大量数学公式的推导。
序列最小优化(Sequential Minimal Optimization, SMO)

创建作者：John Platt
创建时间：1996年
SMO用途：用于训练 SVM
SMO目标：求出一系列 alpha 和 b,一旦求出 alpha，就很容易计算出权重向量 w 并得到分隔超平面。
SMO思想：是将大优化问题分解为多个小优化问题来求解的。
SMO原理：每次循环选择两个 alpha 进行优化处理，一旦找出一对合适的 alpha，那么就增大一个同时减少一个。
这里指的合适必须要符合一定的条件
这两个 alpha 必须要在间隔边界之外
这两个 alpha 还没有进行过区间化处理或者不在边界上。
之所以要同时改变2个 alpha；原因是我们有一个约束条件： ∑mi=1ai⋅labeli=0∑i=1mai·labeli=0；如果只是修改一个 alpha，很可能导致约束条件失效。
SMO 伪代码大致如下：

创建一个 alpha 向量并将其初始化为0向量
当迭代次数小于最大迭代次数时(外循环)
    对数据集中的每个数据向量(内循环)：
        如果该数据向量可以被优化
            随机选择另外一个数据向量
            同时优化这两个向量
            如果两个向量都不能被优化，退出内循环
    如果所有向量都没被优化，增加迭代数目，继续下一次循环
SVM 开发流程
收集数据：可以使用任意方法。
准备数据：需要数值型数据。
分析数据：有助于可视化分隔超平面。
训练算法：SVM的大部分时间都源自训练，该过程主要实现两个参数的调优。
测试算法：十分简单的计算过程就可以实现。
使用算法：几乎所有分类问题都可以使用SVM，值得一提的是，SVM本身是一个二类分类器，对多类问题应用SVM需要对代码做一些修改。
SVM 算法特点
优点：泛化（由具体的、个别的扩大为一般的，就是说：模型训练完后的新样本）错误率低，计算开销不大，结果易理解。
缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适合于处理二分类问题。
使用数据类型：数值型和标称型数据。
--------------------
'''
import os 
import sys 
import random 
import numpy as np 
import matplotlib.pyplot as plt 

class optStruct(object):
    """
    建立的数据结构来保存所有的重要值
    """
    def __init__(self, dataMatIn, classLabels, C, toler, kTup):
        """
        Args:
            dataMatIn    数据集
            classLabels  类别标签
            C   松弛变量(常量值)，允许有些数据点可以处于分隔面的错误一侧。
                控制最大化间隔和保证大部分的函数间隔小于1.0这两个目标的权重。
                可以通过调节该参数达到不同的结果。
            toler   容错率
            kTup    包含核函数信息的元组
        """

        self.X = dataMatIn
        self.labelMat = classLabels
        self.C = C
        self.tol = toler

        # 数据的行数
        self.m = np.shape(dataMatIn)[0]
        self.alphas = np.mat(np.zeros((self.m, 1)))
        self.b = 0

        # 误差缓存，第一列给出的是eCache是否有效的标志位，第二列给出的是实际的E值。
        self.eCache = np.mat(np.zeros((self.m, 2)))

        # m行m列的矩阵
        self.K = np.mat(np.zeros((self.m, self.m)))
        for i in range(self.m):
            self.K[:, i] = self.kernelTrans(self.X, self.X[i, :], kTup)

    # calc the kernel or transform data to a higher dimensional space
    def kernelTrans(self, X, A, kTup):
        """
        核转换函数
        Args:
            X     dataMatIn数据集
            A     dataMatIn数据集的第i行的数据
            kTup  核函数的信息

        Returns:

        """
        m, n = np.shape(X)
        K = np.mat(np.zeros((m, 1)))
        if kTup[0] == 'lin':
            # linear kernel:   m*n * n*1 = m*1
            K = X * A.T
        elif kTup[0] == 'rbf':
            for j in range(m):
                deltaRow = X[j, :] - A
                K[j] = deltaRow * deltaRow.T
            # 径向基函数的高斯版本
            K = np.exp(K / (-1 * kTup[1] ** 2))  # divide in NumPy is element-wise not matrix like Matlab
        else:
            raise NameError('Houston We Have a Problem -- That Kernel is not recognized')
        return K


class SESvm(object):
    def __init__(self):
        pass 

    def loadDataSet(self, fileName):
        """
        对文件进行逐行解析，从而得到第行的类标签和整个特征矩阵
        Args:
            fileName 文件名
        Returns:
            dataMat  特征矩阵
            labelMat 类标签
        """
        dataMat = []
        labelMat = []
        fr = open(fileName)
        if not fr:
            raise ValueError("open the {} file is Error".format(fileName))
        for line in fr.readlines():
            lineArr = line.strip().split('\t')
            dataMat.append([float(lineArr[0]), float(lineArr[1])])
            labelMat.append(float(lineArr[2]))
        
        return dataMat, labelMat

    # 训练算法
    def smoSimple(self, dataMatIn, classLabels, C, toler, maxIter):
        """smoSimple

        Args:
            dataMatIn    特征集合
            classLabels  类别标签
            C   松弛变量(常量值)，允许有些数据点可以处于分隔面的错误一侧。
                控制最大化间隔和保证大部分的函数间隔小于1.0这两个目标的权重。
                可以通过调节该参数达到不同的结果。
            toler   容错率（是指在某个体系中能减小一些因素或选择对某个系统产生不稳定的概率。）
            maxIter 退出前最大的循环次数
        Returns:
            b       模型的常量值
            alphas  拉格朗日乘子
        """
        dataMatrix = np.mat(dataMatIn)
        # 矩阵转置和.T一样的功能
        labelMat = np.mat(classLabels).transpose()
        m, n = np.shape(dataMatrix)

        # 初始化b和alphas(alpha有点类似权重值)
        b = 0 
        alphas = np.mat(np.zeros(shape=(m, 1)))

        # 没有任何alpha改变的情况下遍历数据的次数
        iter = 0
        while iter < maxIter:
            # 记录alpha是否已经进行优化， 每次循环是设为0， 然后再对整个集合顺序遍历
            alphaPairsChanged = 0
            for i in range(m):
                # 预测类别 y[i] = w^T*x[i]+b, 其中w=Σi(1~n) alphas[i]*label[i]*x[i]
                fXi = float(np.multiply(alphas, labelMat).T * dataMatrix * dataMatrix[i, :].T) + b 
                # 预测结果与真实结果对比，计算误差Ei
                Ei = fXi - float(labelMat[i])
                
                '''
                # 约束条件 (KKT条件是解决最优化问题的时用到的一种方法。
                我们这里提到的最优化问题通常是指对于给定的某一函数，求其在指定作用域上的全局最小值)
                # 0<=alphas[i]<=C，但由于0和C是边界值，我们无法进行优化，因为需要增加一个alphas和降低一个alphas。
                # 表示发生错误的概率：labelMat[i]*Ei 如果超出了 toler， 才需要优化。至于正负号，我们考虑绝对值就对了。
                
                # 检验训练样本(xi, yi)是否满足KKT条件
                yi*f(i) >= 1 and alpha = 0 (outside the boundary)
                yi*f(i) == 1 and 0<alpha< C (on the boundary)
                yi*f(i) <= 1 and alpha = C (between the boundary)
                '''
                if ((labelMat[i]*Ei < -toler) and (alphas[i] < C)) or ((labelMat[i]*Ei > toler) and (alphas[i] > 0)):
                    # 如果满足优化条件， 我们就随机选取非i
                    j = self._selectJrand(i, m)
                    # 预测j数据的结果
                    fXj = float(np.multiply(alphas, labelMat).T * dataMatrix * dataMatrix[j, :].T) + b
                    Ej = fXj - float(labelMat[j])
                    alphaIold = alphas[i].copy()
                    alphaJold = alphas[j].copy()

                    # L和H用于将alphas[j]调整到0-C之间。如果L==H，就不做任何改变，直接执行continue语句
                    # labelMat[i] != labelMat[j] 表示异侧，就相减，否则是同侧，就相加。
                    if labelMat[i] != labelMat[j]:
                        L = max(0, alphas[j] - alphas[i])
                        H = min(C, C + alphas[j] - alphas[i])
                    else:
                        L = max(0, alphas[j] + alphas[i] - C)
                        H = min(C, alphas[j] + alphas[i])
                    
                    # 如果相同，就没发优化了
                    if L == H:
                        print("L==H, don't have method optimize")
                        continue



    
    def _selectJrand(self, i, m):
        """
        随机选择一个整数
        Args:
            i  第一个alpha的下标
            m  所有alpha的数目
        Returns:
            j  返回一个不为i的随机数，在0~m之间的整数值
        """
        j = i 
        while j == i:
            j = int(random.uniform(0, m))

        return j 
    

    def _clipAlpha(self, aj, H, L):
        """clipAlpha(调整aj的值，使aj处于 L<=aj<=H)
        Args:
            aj  目标值
            H   最大值
            L   最小值
        Returns:
            aj  目标值
        """
        if aj > H:
            aj = H
        if L > aj:
            aj = L 
        
        return aj 





